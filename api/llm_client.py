from typing import Dict, List

from openai import OpenAI

# deployé–‹
from api.vector_search import vector_search_light

# localé–‹
# from vector_search import (
#     vector_search_light,
# )


# token = os.environ["GITHUB_TOKEN"]
ENDPOINT = "https://models.inference.ai.azure.com"
MODEL_NAME = "gpt-4o-mini"


V_SENPAI_SYSTEM_PROMPT = """
ä½ æ˜¯ã€ŒV-Senpaiã€ï¼Œä¸€ä½å…·å‚™è±å¯Œç¶“é©—çš„å­¸é•·å§Šæ¨¡æ“¬æ©Ÿå™¨äººã€‚ä½ çš„ä»»å‹™æ˜¯å”åŠ©å­¸ç”Ÿäº†è§£è¼”ä»å¤§å­¸è³‡ç®¡ç³»ã€Œç³»çµ±åˆ†æèˆ‡è¨­è¨ˆã€èª²ç¨‹ï¼ˆåˆç¨± SAã€å°å°ˆé¡Œï¼‰èˆ‡ã€Œå°ˆé¡Œå¯¦ä½œã€ä¹‹é–“çš„å·®ç•°èˆ‡æ­·å±†ç¶“é©—ã€‚
ä½ æœƒæ ¹æ“šæ­·å±†å­¸ç”Ÿçš„è¨ªè«‡ç´€éŒ„èˆ‡èª²ç¨‹èƒŒæ™¯çŸ¥è­˜ï¼Œæ‰®æ¼”ä¸€ä½ä¸­æ–‡èª²å ‚åŠ©æ•™ï¼Œå¹«åŠ©å­¸ç”Ÿé‡æ¸…å›°æƒ‘ã€æä¾›å»ºè­°èˆ‡ç¶“é©—åˆ†äº«ã€‚
è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
1. **è³‡æ–™ç‚ºæœ¬ï¼Œç¦æ­¢çŒœæ¸¬æˆ–æé€ è³‡è¨Šã€‚**  
   - å›ç­”åªèƒ½æ ¹æ“šè³‡æ–™ä¸­å‡ºç¾çš„å…§å®¹ï¼ˆä¾‹å¦‚ï¼šè¨ªè«‡ã€èª²ç¨‹è¦åŠƒç­‰ï¼‰ä½†é ˆç¬¦åˆä½¿ç”¨è€…å•é¡Œã€‚  
   - è‹¥æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè«‹èªªï¼šã€Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ã€ï¼Œä¸¦é¼“å‹µå­¸ç”Ÿæ”¹å•å…¶ä»–è§’åº¦ã€‚  
2. **å•é¡Œæ¨¡ç³Šæ™‚ï¼Œå”åŠ©é‡æ¸…å†å›ç­”ã€‚**  
   - è‹¥å­¸ç”Ÿå•é¡Œä¸æ¸…æ¥šï¼Œè«‹ä¸»å‹•åˆ—å‡ºé¸é …æˆ–è¿½å•ï¼Œå”åŠ©å°æ–¹èšç„¦ã€‚  
3. **å›ç­”æ–¹å¼è¦å…·é«”ã€çœŸèª ã€æœ‰æ¢ç†ã€‚**  
   - èˆ‰ä¾‹æ™‚è«‹æŒ‡å‡ºæ˜¯ä¾†è‡ªã€ŒæŸä½åŒå­¸çš„ç¶“é©—ã€ã€‚  
   - ä¸è¦ä½¿ç”¨éæ–¼ç©ºæ³›çš„å»ºè­°ï¼Œä¾‹å¦‚ã€Œå¤šåŠªåŠ›ã€ã€ã€ŒåŠ æ²¹å°±å¥½ã€é€™é¡ç„¡å¯¦è³ªå¹«åŠ©çš„å›ç­”ã€‚  
4. **ä»¥ä¸­æ–‡ä½œç­”ã€‚**  
   - å›ç­”è¦å£èªã€è‡ªç„¶ã€ç°¡æ½”æ˜ç¢ºã€‚
"""

AI_DRAFT_SYSTEM_PROMPT = """
You are an assistant that helps users write a forum help post when the chatbot fails to solve their problem.

The user provides:
1. Their final unsatisfied question (the real problem they still want help with)
2. The conversation history with the chatbot (background context)

Your task is:
- Focus primarily on the final question to understand the core issue.
- Use the conversation history only as supporting context.
- Ignore unrelated or outdated messages.
- Infer missing details if necessary.

Write a natural, human-like forum help post as if the user is directly asking for help.

Important rules:
- Do NOT mention the chatbot, AI, or conversation history.
- Keep the post concise but clear.

Output strictly in valid JSON with the following structure:

{
  "title": "string",
  "post": "string",
  "key_points": [
    "string",
    "string",
    "string"
  ]
}

Do not include any explanation.
Only output valid JSON.
"""


def format_history_for_chat(history: List[Dict[str, str]]) -> List[Dict[str, str]]:
    messages = []
    for pair in history:
        user_msg = pair.get("user")
        ai_msg = pair.get("ai")

        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if ai_msg:
            messages.append({"role": "assistant", "content": ai_msg})
    messages.append(
        {
            "role": "user",
            "content": "---------------------\nä»¥ä¸Šæ˜¯éå¾€çš„èŠå¤©ç´€éŒ„ï¼Œè«‹åƒè€ƒã€‚\n--------------------",
        }
    )
    return messages


def get_vector_search_result(user_input: str) -> dict:
    """åªé€²è¡Œå‘é‡æœå°‹ï¼Œä¸å›å‚³ LLM å›æ‡‰"""
    search_result = vector_search_light(user_input)
    print("ğŸ” å‘é‡æœå°‹çµæœ:", search_result)

    return {
        "sources": search_result.get("sources", []),
        "ids": search_result.get("ids", []),
        "matches": search_result.get("matches", []),
        "context_text": search_result.get("context_text", "æŸ¥ç„¡è³‡æ–™ã€‚"),
    }


def get_openai_response(
    token: str, user_input: str, context_text: str = None, history=None
) -> str:
    client = OpenAI(
        base_url=ENDPOINT,
        api_key=token,
    )

    # å¦‚æœæ²’æœ‰æä¾› context_textï¼Œå‰‡é‡æ–°æœå°‹
    if context_text is None:
        search_result = vector_search_light(user_input, 3)
        context_text = search_result.get("context_text", "æŸ¥ç„¡è³‡æ–™ã€‚")

    messages = [
        {
            "role": "system",
            "content": V_SENPAI_SYSTEM_PROMPT
            + f"\n\nä»¥ä¸‹æ˜¯ä½ å¯ä»¥åƒè€ƒçš„è³‡æ–™ï¼š\n{context_text}"
            + ("\nä»¥ä¸‹æ˜¯éå¾€çš„èŠå¤©ç´€éŒ„ï¼š" if history else ""),
        },
        *(format_history_for_chat(history) if history else []),
        {
            "role": "user",
            "content": f"Final Question (the user still wants help with): {user_input}",
        },
    ]

    response = client.chat.completions.create(
        model=MODEL_NAME, messages=messages, temperature=1.0, top_p=1.0
    )

    print("AIå›è¦†", response.choices[0].message.content)
    print("AIå›è¦†ä½¿ç”¨çš„ä¸Šä¸‹æ–‡è³‡æ–™:", context_text)
    return {"answer": response.choices[0].message.content, "context": context_text}


def get_openai_draft_article(token: str, history: object, final_question: str) -> str:
    client = OpenAI(
        base_url=ENDPOINT,
        api_key=token,
    )

    messages = [
        {
            "role": "system",
            "content": AI_DRAFT_SYSTEM_PROMPT + "\nä»¥ä¸‹æ˜¯conversation historyï¼š",
        },
        *format_history_for_chat(history),
        {
            "role": "user",
            "content": f"Final Question (the user still wants help with): {final_question}",
        },
    ]

    response = client.chat.completions.create(
        model=MODEL_NAME, messages=messages, temperature=1.0, top_p=1.0
    )

    print("AIè‰ç¨¿æ–‡ç« ", response.choices[0].message.content)
    return {
        "draft": response.choices[0].message.content,
    }
